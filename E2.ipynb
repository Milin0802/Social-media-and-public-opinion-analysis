{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本摘要\n",
    "* 加载数据\n",
    "* 建立Textrank 模型\n",
    "* 测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 17527.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Stories 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from load_data import load_stories\n",
    "\n",
    "directory = 'data/cnn_stories_tokenized/'\n",
    "stories = load_stories(directory, 10000)\n",
    "print('Loaded Stories %d' % len(stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['story', 'highlights'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'story': '\\n\\n',\n",
       " 'highlights': ['Iggy says Miami is fundamentally spiritual and has a quicksilver quality',\n",
       "  \"It 's a very musical town , with some damn good rock players , he says\",\n",
       "  \"`` Miami 's a sunny place for shady people , '' says the Godfather of Punk\"]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = stories[2584]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'story': '\\n\\n',\n",
       " 'highlights': ['CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery',\n",
       "  'Please submit your best shots of Barcelona , Spain for next week',\n",
       "  'Visit CNN.com / Travel next Wednesday for a new gallery of snapshots']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = stories[3274]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories.remove(a)\n",
    "stories.remove(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4998"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   2 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    \"\"\"Dict that can get attribute by dot\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize ## 分句\n",
    "from nltk.tokenize import word_tokenize ## 分词\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def get_similarity(word_list1, word_list2):\n",
    "        \"\"\"默认的用于计算两个句子相似度的函数。\n",
    "        Keyword arguments:\n",
    "        word_list1, word_list2  --  分别代表两个句子，都是由单词组成的列表\n",
    "        \"\"\"\n",
    "        words   = list(set(word_list1 + word_list2))        \n",
    "        vector1 = [float(word_list1.count(word)) for word in words]\n",
    "        vector2 = [float(word_list2.count(word)) for word in words]\n",
    "\n",
    "        vector3 = [vector1[x]*vector2[x]  for x in range(len(vector1))]\n",
    "        vector4 = [1 for num in vector3 if num > 0.]\n",
    "        co_occur_num = sum(vector4)\n",
    "\n",
    "        if abs(co_occur_num) <= 1e-12:\n",
    "            return 0.\n",
    "\n",
    "        denominator = math.log(float(len(word_list1))) + math.log(float(len(word_list2))) # 分母\n",
    "\n",
    "        if abs(denominator) < 1e-12:\n",
    "            return 0.\n",
    "\n",
    "        return co_occur_num / denominator\n",
    "    \n",
    "class TextRank4Sentence(object):\n",
    "    \n",
    "    def __init__(self, stop_words = None):\n",
    "        \"\"\"\n",
    "        Keyword arguments:\n",
    "        stop_words  --  set，停止词集合 方便查询\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.seg = sent_tokenize\n",
    "        self.stop_words = stop_words\n",
    "        \n",
    "        \n",
    "        self.sentences = None\n",
    "        self.words_no_filter = None     # 2维列表\n",
    "        self.words_no_stop_words = None\n",
    "        self.key_sentences = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    def sort_sentences(self,sentences, words, sim_func = get_similarity, pagerank_config = {'alpha': 0.85,}):\n",
    "        \"\"\"将句子按照关键程度从大到小排序\n",
    "        Keyword arguments:\n",
    "        sentences         --  列表，元素是句子\n",
    "        words             --  二维列表，子列表和sentences中的句子对应，子列表由单词组成\n",
    "        sim_func          --  计算两个句子的相似性，参数是两个由单词组成的列表\n",
    "        pagerank_config   --  pagerank的设置\n",
    "        \"\"\"\n",
    "        sorted_sentences = []\n",
    "        _source = words\n",
    "        sentences_num = len(_source)        \n",
    "        graph = np.zeros((sentences_num, sentences_num))\n",
    "\n",
    "        for x in range(sentences_num):\n",
    "            for y in range(x, sentences_num):\n",
    "                similarity = sim_func( _source[x], _source[y] )\n",
    "                graph[x, y] = similarity\n",
    "                graph[y, x] = similarity\n",
    "\n",
    "        nx_graph = nx.from_numpy_matrix(graph)\n",
    "        scores = nx.pagerank(nx_graph, **pagerank_config)              # this is a dict\n",
    "        sorted_scores = sorted(scores.items(), key = lambda item: item[1], reverse=True)\n",
    "\n",
    "        for index, score in sorted_scores:\n",
    "            item = AttrDict(index=index, sentence=sentences[index], weight=score)\n",
    "            sorted_sentences.append(item)\n",
    "\n",
    "        return sorted_sentences\n",
    "        \n",
    "    def analyze(self, text, lower = False, \n",
    "              source = 'no_filter', \n",
    "              sim_func = get_similarity,\n",
    "              pagerank_config = {'alpha': 0.85,}):\n",
    "        \"\"\"\n",
    "        Keyword arguments:\n",
    "        text                 --  文本内容，字符串。\n",
    "        lower                --  是否将文本转换为小写。默认为False。\n",
    "        source               --  选择使用words_no_filter, words_no_stop_words, words_all_filters中的哪一个来生成句子之间的相似度。\n",
    "                                 默认值为`'all_filters'`，可选值为`'no_filter', 'no_stop_words', 'all_filters'`。\n",
    "        sim_func             --  指定计算句子相似度的函数。\n",
    "        \"\"\"\n",
    "        \n",
    "        self.key_sentences = []\n",
    "        \n",
    "        if lower:\n",
    "            text = text.lower()\n",
    "        \n",
    "        result = self.seg(text)\n",
    "        self.sentences = result\n",
    "        \n",
    "        self.words_no_filter = [ word_tokenize(s) for s in self.sentences]\n",
    "        self.words_no_stop_words = []\n",
    "        if self.stop_words:\n",
    "            for words in self.words_no_filter:\n",
    "                self.words_no_stop_words.append([])\n",
    "                for w in words:\n",
    "                    if w not in self.stop_words:\n",
    "                        self.words_no_stop_words[-1].append(w)\n",
    "\n",
    "\n",
    "        options = ['no_filter', 'no_stop_words']\n",
    "        if source in options:\n",
    "            if source ==  \"no_filter\":\n",
    "                _source = self.words_no_filter\n",
    "            else:\n",
    "                _source = self.words_no_stop_words\n",
    "        else:\n",
    "            _source = self.words_no_stop_words\n",
    "        \n",
    "        self.key_sentences = self.sort_sentences(sentences = self.sentences,\n",
    "                                                 words     = _source,\n",
    "                                                 sim_func  = sim_func,\n",
    "                                                 pagerank_config = pagerank_config)\n",
    "\n",
    "            \n",
    "    def get_key_sentences(self, num = 1, sentence_min_len = 0):\n",
    "        \"\"\"获取最重要的num个长度大于等于sentence_min_len的句子用来生成摘要。\n",
    "        Return:\n",
    "        多个句子组成的列表。\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        count = 0\n",
    "        for item in self.key_sentences:\n",
    "            if count >= num:\n",
    "                break\n",
    "            if len(item['sentence']) >= sentence_min_len:\n",
    "                result.append(item)\n",
    "                count += 1\n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr4s = TextRank4Sentence()\n",
    "tr4s.analyze(stories[0][\"story\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"`` Clash was an adult male living a prominent public life centered around the entertainment of toddlers , while at the same time he was , in secret , preying on teenage boys to satisfy his depraved sexual interests , '' the Stephens suit alleged .\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr4s.get_key_sentences()[0][\"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumeval.metrics.rouge import RougeCalculator\n",
    "rouge = RougeCalculator(stopwords=True)\n",
    "\n",
    "def get_rouge(predict,answer):\n",
    "    rouge_p = {1:0.0,2:0.0,3:0.0}\n",
    "    for i in tqdm(range(len(predict))):\n",
    "        rouge_p[1] += rouge.rouge_n(\n",
    "            summary=predict[i],\n",
    "            references=answer[i],\n",
    "            n=1) \n",
    "        rouge_p[2] += rouge.rouge_n(\n",
    "            summary=predict[i],\n",
    "            references=answer[i],\n",
    "            n=2) \n",
    "        rouge_p[3] += rouge.rouge_l(\n",
    "            summary=predict[i],\n",
    "            references=answer[i]) \n",
    "\n",
    "    rouge_p[1] /= len(predict)\n",
    "    rouge_p[2] /= len(predict)\n",
    "    rouge_p[3] /= len(predict)\n",
    "\n",
    "    print(\"ROUGE-1: {}, ROUGE-2: {}, ROUGE-L: {}\".format(\n",
    "    rouge_p[1], rouge_p[2], rouge_p[3]\n",
    "    ).replace(\", \", \"\\n\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stop_words = set( stopwords.words('english'))\n",
    "\n",
    "tr4s = TextRank4Sentence(stop_words = stop_words)\n",
    "tr4s.analyze(stories[3273][\"story\"],source = 'no_stop_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I like to be classic with a modern twist , says fashion designer',\n",
       "  'The key to building a successful brand is separating creative and business sides',\n",
       "  'Style is something different in everyone , Herrera says'],\n",
       " 'On building a business ...\\n\\n-LRB- Fashion is -RRB- a very difficult business , as you know , because fashion is a business .')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(stories[3273][\"highlights\"],tr4s.get_key_sentences()[0][\"sentence\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4998/4998 [14:54<00:00,  5.59it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tr4s_have = TextRank4Sentence()\n",
    "tr4s_no = TextRank4Sentence(stop_words = stop_words)\n",
    "\n",
    "have_stop = []\n",
    "no_stop_words = []\n",
    "answer = []\n",
    "\n",
    "for s in tqdm(stories):\n",
    "    tr4s_have.analyze(s[\"story\"])\n",
    "    tr4s_no.analyze(s[\"story\"],source = 'no_stop_words')\n",
    "    \n",
    "    if len(tr4s_have.get_key_sentences()) != 0 and len(tr4s_no.get_key_sentences()) != 0:\n",
    "        answer.append(s[\"highlights\"])\n",
    "\n",
    "        have_stop.append(tr4s_have.get_key_sentences()[0][\"sentence\"])\n",
    "        no_stop_words.append(tr4s_no.get_key_sentences()[0][\"sentence\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4996/4996 [00:12<00:00, 407.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 0.10220309646469726\n",
      "ROUGE-2: 0.025678254771702522\n",
      "ROUGE-L: 0.09089767869257702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "get_rouge(have_stop,answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4996/4996 [00:11<00:00, 435.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 0.11711719721983674\n",
      "ROUGE-2: 0.0301544862409943\n",
      "ROUGE-L: 0.10366080105375121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "get_rouge(no_stop_words,answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
